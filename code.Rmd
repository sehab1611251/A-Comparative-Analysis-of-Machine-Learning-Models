---
title: "Predicting National Happiness Levels: A Comparative Analysis of Machine Learning Models"
author: "Md Sehabub Zaman Pranta"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
---

```{r}
options(repos = c(CRAN = "https://cran.rstudio.com/")) # Set a CRAN mirror
```

```{r}
knitr::opts_chunk$set(fig.align='center')
```

```{r}
knitr::opts_chunk$set(fig.width=7.5, fig.height=4.5, echo=TRUE, size="small")
```


```{r}
tinytex::install_tinytex(force = TRUE)
```



```{r library_chunk, echo = TRUE, results = 'hide'}
# Import necessary libraries

install.packages("glmnet")
install.packages("corrplot")

library(tidyverse)  # For data manipulation and visualization
library(readxl)     # For reading Excel files
library(caret) #install.packages("caret")
library(nnet) # For multinomial logistic regression
library(glmnet) # For Ridge regression
library(pROC) # Load the pROC library for ROC analysis
library(e1071) # Naive Bayes library
```

# Introduction
National happiness serves as a crucial indicator of societal progress and development. Understanding the factors that contribute to national happiness can provide valuable insights for shaping policies aimed at improving overall well-being.\
The current project aims to predict national happiness levels using five distinct machine learning models: Multiple Linear Regression, Multinomial Logistic Regression, Ridge Regression, Polynomial Regression, and Naive Bayes. The primary objective is to evaluate the efficacy of these models in forecasting the Life Ladder score, a self-reported measure of life satisfaction that serves as the national happiness in this context. The project further extends its predictive capabilities by classifying countries into various happiness levels, ranging from "very unhappy" to "very happy," based on socioeconomic and well-being indicators.\
Multiple Linear Regression establishes a linear relationship between predictor variables and the continuous Life Ladder score. Multinomial Logistic Regression, a versatile statistical model, tackles the multi-class classification problem of predicting the categorical Happiness_Level. Ridge Regression, a regularization technique applied to linear Regression, enhances model stability and performance by addressing potential multicollinearity issues among predictors. Polynomial Regression extends the capacity of linear Regression by capturing non-linear relationships in the data. Finally, Naive Bayes provides a probabilistic framework for classifying happiness levels by modeling the likelihood of predictor values under each class.\
By meticulously comparing the performance of these five models, this project seeks to identify the most suitable approach for predicting national happiness and the factors that influence it.


# Dataset Processing
This section focuses on preparing the [World Happiness Report](https://worldhappiness.report/data/) data for subsequent analysis and modeling.

## Data Import and Cleaning
The data is imported from an Excel file using the read_excel function. The column names are then cleaned and standardized by removing spaces and replacing them with underscores. The summary function provides an overview of the data, including descriptive statistics and highlighting the presence of missing values. The str function displays the structure of the dataset, showing the data type of each column. The 'year' column is converted to a factor, recognizing its categorical nature. Missing values are identified and removed from the dataset to ensure data integrity.\

```{r}
#################### Step 1: Dataset Processing ################

happiness_data <- read_excel("/Users/pranta/Desktop/SIL/Data.xls") # Import data
head(happiness_data)
```

```{r}
# The column names have spaces, which might make them a bit cumbersome to work with.
# So, renaming the columns.
happiness_data <- happiness_data %>% 
  rename(
    Country_name = `Country name`,
    Life_Ladder = `Life Ladder`,
    Log_GDP_per_capita = `Log GDP per capita`,
    Social_support = `Social support`,
    Healthy_life_expectancy = `Healthy life expectancy at birth`,
    Freedom_to_make_life_choices = `Freedom to make life choices`,
    Perceptions_of_corruption = `Perceptions of corruption`,
    Positive_affect = `Positive affect`,
    Negative_affect = `Negative affect`
  )

head(happiness_data)
```

```{r}
# Get a summary of each column
summary(happiness_data)
```


```{r}

# Check the data types of each column
str(happiness_data)

# Check the dimension
dim(happiness_data)

# Convert Year to Factor
happiness_data$year <- as.factor(happiness_data$year)

# Count missing values in each column
colSums(is.na(happiness_data))

# Remove rows with missing values in any column
happiness_data_cleaned <- na.omit(happiness_data)

# Check dimensions of the cleaned dataset
dim(happiness_data_cleaned)
```

## Data Visualization
The cleaned dataset is visualized using histograms, boxplots, and scatter plots. A histogram illustrates the distribution of the target variable, 'Life_Ladder.' Boxplots are generated for each predictor variable to identify potential outliers. Scatter plots visualize the relationships between each predictor and the 'Life_Ladder.'

```{r}
#.........Data Visualization..........#

# Histogram of Life_Ladder. To visualize the distribution of target variable.
ggplot(happiness_data_cleaned, aes(x = Life_Ladder)) +
  geom_histogram(bins = 20, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Life_Ladder", x = "Life_Ladder", y = "Frequency")
```

```{r}
# Boxplots for All Predictors. To identify outliers in each predictor variable.
predictors <- c("Log_GDP_per_capita", "Social_support", "Healthy_life_expectancy", 
                "Freedom_to_make_life_choices", "Generosity", 
                "Perceptions_of_corruption", "Positive_affect", "Negative_affect")
for (predictor in predictors) {
  plot <- ggplot(happiness_data_cleaned, aes_string(y = predictor)) +
    geom_boxplot(fill = "lightgreen", color = "black") +
    labs(title = paste("Boxplot of", predictor), y = predictor) +
    theme_minimal()
  print(plot)
}
```



```{r}
# Scatter Plots of All Predictors vs. Life_Ladder. To explore relationships between predictor variables and the response variable (Life_Ladder).
for (predictor in predictors) {
  plot <- ggplot(happiness_data_cleaned, aes_string(x = predictor, y = "Life_Ladder")) +
    geom_point(color = "blue") +
    labs(title = paste("Scatter Plot of", predictor, "vs Life_Ladder"), x = predictor, y = "Life_Ladder") +
    theme_minimal()
  print(plot)
}

```

## Target Variable Creation for Happiness Prediction
Here, I calculated the median of the Life_Ladder variable and created a new categorical variable, Happiness_Level, by dividing Life_Ladder scores into ordered categories that represent varying degrees of happiness, from "very unhappy" to "very happy." Each category is assigned a range based on the Life_Ladder score. The new Happiness_Level variable is then converted into a factor with ordered levels to facilitate classification tasks in the analysis.

```{r}
# Create the Categorical Target Variable `Happiness_Level` with 4 classes
happiness_data_cleaned <- happiness_data_cleaned %>%
  mutate(Happiness_Level = case_when(
    Life_Ladder <= 4.5 ~ "very_unhappy",       
    Life_Ladder > 4.5 & Life_Ladder <= 5.5 ~ "unhappy",      
    Life_Ladder > 5.5 & Life_Ladder <= 6.5 ~ "happy",     
    Life_Ladder > 6.5 ~ "very_happy"          
  ))

# Convert Happiness_Level to factor
happiness_data_cleaned$Happiness_Level <- factor(
  happiness_data_cleaned$Happiness_Level,
  levels = c("very_unhappy", "unhappy", "happy", "very_happy")
)

# Display the first few rows to confirm changes
head(happiness_data_cleaned)
```
## Data Splitting 
The dataset is split into training (80%) and testing (20%) sets using createDataPartition. The distribution of 'Happiness_Level' is checked in both sets to ensure balanced representation.

```{r}
# Check the distribution of Happiness_Level
table(happiness_data_cleaned$Happiness_Level)

cat("\n\n")

# Check new dimensions after adding target column Happiness_Level
dim(happiness_data_cleaned)

cat("\n\n")

# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(happiness_data_cleaned$Happiness_Level, p = 0.8, list = FALSE)
happiness_data_train <- happiness_data_cleaned[train_index, ]
happiness_data_test <- happiness_data_cleaned[-train_index, ]

# Check the distribution in training and testing sets
print("Train:")
table(happiness_data_train$Happiness_Level)
cat("\n")
print("Test:")
table(happiness_data_test$Happiness_Level)
```
## Influential Points Detection and Removal
A linear model is fitted to the training data, and Cookâ€™s distance is calculated to identify influential points. These influential points, which may include outliers, are removed from the training set to improve model robustness.

```{r}
#.........Outlier Detection using Cook's Distance.........#

# Fit a linear model to calculate Cook's Distance
lm_model <- lm(Life_Ladder ~ Social_support + Healthy_life_expectancy +
                 Freedom_to_make_life_choices + Perceptions_of_corruption + 
                 Positive_affect + Negative_affect, data = happiness_data_train)

# Calculate Cook's Distance
cooks_distance <- cooks.distance(lm_model)

# Identify influential points (e.g., Cook's Distance > 4/n)
influential_points <- which(cooks_distance > (4 / nrow(happiness_data_train)))

# Plot influential points on the Cook's distance plot
plot(cooks_distance, pch="*", cex=2, main="Influential Observations by Cook's Distance")  
points(influential_points, cooks_distance[influential_points], col="red", pch=19, cex=2)
# Add labels to the influential points (see their row numbers)
text(influential_points, cooks_distance[influential_points], labels=influential_points, pos=3, cex=0.8)
# Add a legend to explain the red points
legend("topright", legend=c("Influential Points"), col="red", pch=19, cex=0.8)

# Remove influential points from the training set
happiness_data_train_cleaned <- happiness_data_train[-influential_points, ]
```

# Model Building (Applying Supervised Models)  
This section involves training and evaluating three different supervised machine learning models to predict happiness levels: Multiple Linear Regression, Multinomial Logistic Regression, and Ridge Regression.

## Multiple Linear Regression
The first model employed is Multiple Linear Regression, which aims to establish a linear relationship between the 'Life_Ladder' (the continuous target variable) and the selected predictor variables. The lm() function is used to train the model on the cleaned training data, and the summary() function provides insights into the model's coefficients, their standard errors, t-values, and p-values. The Adjusted R-squared value indicates the proportion of variance in the 'Life_Ladder' explained by the model. The standard errors and p-values help assess the statistical significance of each predictor's contribution to the model.

```{r}
############ Step 2: Model Building (Applying Supervised Models) ############

### Model 1: Multiple Linear Regression ###

# Train Multiple Linear Regression Model
model_multiple_linear <- lm(Life_Ladder ~ Social_support + Healthy_life_expectancy +
                              Freedom_to_make_life_choices + Perceptions_of_corruption + 
                              Positive_affect + Negative_affect,
                            data = happiness_data_train_cleaned)

# Print model summary
summary(model_multiple_linear)

# Extract Adjusted R-squared from model summary
adjusted_r_squared_multiple_linear <- summary(model_multiple_linear)$adj.r.squared

# Calculate AIC
aic_multiple_linear <- AIC(model_multiple_linear)

# Extract standard errors and p-values
std_errors <- summary(model_multiple_linear)$coefficients[, "Std. Error"]
p_values <- summary(model_multiple_linear)$coefficients[, "Pr(>|t|)"]

# Print standard errors and p-values
cat("Standard Errors:\n")
print(std_errors)
cat("P-values:\n")
print(p_values)
```

## Multinomial Logistic Regression
The second model, Multinomial Logistic Regression, is suitable for predicting the categorical target variable 'Happiness_Level', which has multiple levels. The multinom() function is used for training, and the summary() function again provides model coefficients, standard errors, and other relevant statistics. The model's performance is evaluated using metrics like accuracy, precision, recall, and F1-score, which are derived from the confusion matrix. Additionally, ROC curves are plotted for each class to visualize the model's ability to discriminate between different happiness levels.

```{r}
### Model 2: Multinomial Logistic Regression ###

# Train Multinomial Logistic Regression Model
model_multinomial <- multinom(Happiness_Level ~ Social_support + Healthy_life_expectancy +
                                Freedom_to_make_life_choices + Perceptions_of_corruption + 
                                Positive_affect + Negative_affect,
                              data = happiness_data_train_cleaned)

# Print model summary
summary(model_multinomial)

# Extract standard errors and p-values
std_errors_multinomial <- summary(model_multinomial)$standard.errors
p_values_multinomial <- summary(model_multinomial)$coefficients / std_errors_multinomial
p_values_multinomial <- 2 * (1 - pnorm(abs(p_values_multinomial)))

# Print p-values
cat("Multinomial Logistic Regression P-values:\n")
print(p_values_multinomial)
```

## Ridge Regression
The third model, Ridge Regression, is a regularization technique applied to linear regression to address potential multicollinearity issues and improve model generalization. The "glmnet()" function is used with alpha = 0 to specify Ridge Regression. Cross-validation is performed using "cv.glmnet()" to determine the optimal lambda value, which controls the degree of regularization. The final Ridge Regression model is then trained using the best lambda value. It is valuable for its ability to improve model stability and performance.

```{r}
##### Model 3: Ridge Regression #####

# Prepare the data for Ridge Regression
x_train_cleaned <- model.matrix(Life_Ladder ~ Social_support + Healthy_life_expectancy +
                                  Freedom_to_make_life_choices + Perceptions_of_corruption + 
                                  Positive_affect + Negative_affect, 
                                data = happiness_data_train_cleaned)[, -1]
y_train_cleaned <- happiness_data_train_cleaned$Life_Ladder

# Fit the Ridge Regression model
ridge_model <- glmnet(x_train_cleaned, y_train_cleaned, alpha = 0)

# Perform cross-validation to find the optimal lambda
cv_ridge <- cv.glmnet(x_train_cleaned, y_train_cleaned, alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min

# Print the best lambda value
print(best_lambda_ridge)

# Fit the Ridge Regression model with the best lambda
ridge_model_best <- glmnet(x_train_cleaned, y_train_cleaned, alpha = 0, lambda = best_lambda_ridge)

##### Ridge Regression Coefficients, RMSE, and R^2 #####

# Extract Ridge Regression Coefficients
ridge_coefficients <- coef(ridge_model_best)  # Extract coefficients for optimal lambda
cat("\nRidge Coefficients (All Values):\n")
print(ridge_coefficients)

# Filter non-zero coefficients for Ridge Regression
ridge_non_zero_coefficients <- ridge_coefficients[ridge_coefficients != 0]
cat("\nRidge Coefficients (Non-Zero Values):\n")
print(ridge_non_zero_coefficients)

# Prepare the test data for Ridge Regression (Fix)
x_test_cleaned <- model.matrix(Life_Ladder ~ Social_support + Healthy_life_expectancy +
                                  Freedom_to_make_life_choices + Perceptions_of_corruption + 
                                  Positive_affect + Negative_affect, 
                                data = happiness_data_test)[, -1]

# Calculate RMSE for Ridge Regression
predictions_ridge <- predict(ridge_model_best, s = best_lambda_ridge, newx = x_test_cleaned)
ridge_rmse <- sqrt(mean((predictions_ridge - happiness_data_test$Life_Ladder)^2))
cat("\nRidge RMSE:", ridge_rmse, "\n")

# Calculate R^2 for Ridge Regression
ss_total <- sum((happiness_data_test$Life_Ladder - mean(happiness_data_test$Life_Ladder))^2)  # Total sum of squares
ss_residual <- sum((happiness_data_test$Life_Ladder - predictions_ridge)^2)  # Residual sum of squares
ridge_r_squared <- 1 - (ss_residual / ss_total)
cat("\nRidge R^2:", ridge_r_squared, "\n")


```

# Assess Model Performance (Evaluation of Results)
This part evaluates the predictive capabilities of the three models trained in the previous step: Multiple Linear Regression, Multinomial Logistic Regression, and Ridge Regression. The evaluation uses different metrics to measure the nature of each model's predictions.

## Multiple Linear Regression Assessment
The performance of the Multiple Linear Regression model is measured using two primary metrics: Root Mean Squared Error (RMSE) and R-squared. The RMSE quantifies the average difference between the model's predicted 'Life_Ladder' values and the actual values in the test dataset. A lower RMSE signifies better predictive accuracy. The R-squared value, on the other hand, represents the proportion of variance in the 'Life_Ladder' explained by the model. A higher R-squared indicates a better fit.

```{r}
############ Step 3: Assess Model Performance (Evaluation of Results) #############

### Model Assessment: Multiple Linear Regression ###

# Predictions on the test set
predictions_multiple_linear <- predict(model_multiple_linear, newdata = happiness_data_test)

# Calculate RMSE (Root Mean Squared Error)
rmse_multiple_linear <- sqrt(mean((predictions_multiple_linear - happiness_data_test$Life_Ladder)^2))

# Display Evaluation Metrics for Multiple Linear Regression
cat("\nMultiple Linear Regression Metrics:\n")
cat("RMSE:", rmse_multiple_linear, "\n")
cat("Adjusted R-squared:", adjusted_r_squared_multiple_linear, "\n") # Adjusted R^2
cat("AIC:", aic_multiple_linear, "\n") # AIC
```
## Multinomial Logistic Regression Assessment
This model's performance is evaluated using a confusion matrix, which provides a detailed breakdown of the model's predictions across different 'Happiness_Level' categories. The confusion matrix reveals the number of correct and incorrect predictions for each class, enabling the calculation of metrics like accuracy, precision, recall, and F1-score. Accuracy measures the overall proportion of correct predictions. Precision focuses on the accuracy of positive predictions for each class, while recall quantifies the model's ability to identify all actual positive cases within each class. The F1-score strikes a balance between precision and recall, providing a comprehensive measure of the model's performance for each 'Happiness_Level.' Additionally, ROC curves are plotted for each class, visually depicting the trade-off between true positive rate (sensitivity) and false positive rate (1-specificity) at different classification thresholds. The area under the ROC curve (AUC) serves as a summary metric of the model's discriminatory power.

```{r}
### Model Assessment: Multinomial Logistic Regression ###

# Predict probabilities for each class
probabilities_multinomial <- predict(model_multinomial, newdata = happiness_data_test, type = "probs")

# Predict classes for the test set
predictions_multinomial <- predict(model_multinomial, newdata = happiness_data_test)

# Create confusion matrix
confusionMatrix_multinomial <- table(Predicted = predictions_multinomial, Reference = happiness_data_test$Happiness_Level)
print(confusionMatrix_multinomial)

# Calculate accuracy
accuracy_multinomial <- mean(predictions_multinomial == happiness_data_test$Happiness_Level)

# Calculate Precision, Recall, and F1-Score for each class
precision_multinomial <- diag(confusionMatrix_multinomial) / rowSums(confusionMatrix_multinomial)
recall_multinomial <- diag(confusionMatrix_multinomial) / colSums(confusionMatrix_multinomial)
f1_score_multinomial <- 2 * precision_multinomial * recall_multinomial / (precision_multinomial + recall_multinomial)

# Display Evaluation Metrics for Multinomial Logistic Regression
cat("\nMultinomial Logistic Regression Metrics:\n")
cat("Accuracy:", accuracy_multinomial, "\n")
cat("Precision:\n")
print(round(precision_multinomial, 3))
cat("Recall:\n")
print(round(recall_multinomial, 3))
cat("F1-Score:\n")
print(round(f1_score_multinomial, 3))

# Convert Happiness_Level to numeric for ROC analysis
happiness_data_test$Happiness_Level_numeric <- as.numeric(happiness_data_test$Happiness_Level)

# Initialize a list to store ROC curves
roc_curves <- list()

# Plot ROC curves for each class
par(mfrow = c(2, 3))  # Set up the plotting area to have 2 rows and 3 columns
for (i in 1:nlevels(happiness_data_test$Happiness_Level)) {
  roc_curves[[i]] <- roc(happiness_data_test$Happiness_Level_numeric == i, probabilities_multinomial[, i])
  plot(roc_curves[[i]], main = paste("ROC Curve -", levels(happiness_data_test$Happiness_Level)[i]), print.auc = TRUE)
}
```

## Ridge Regression Assessment
The Ridge Regression model's performance is evaluated primarily using RMSE, similar to the Multiple Linear Regression model. The RMSE is calculated by comparing the model's predicted 'Life_Ladder' values with the actual values in the test dataset.

```{r}
### Model Assessment: Ridge Regression ###
# Predict on the test set
x_test <- model.matrix(Life_Ladder ~ Social_support + Healthy_life_expectancy +
                         Freedom_to_make_life_choices + Perceptions_of_corruption + 
                         Positive_affect + Negative_affect, 
                       data = happiness_data_test)[, -1]
ridge_predictions <- predict(ridge_model_best, s = best_lambda_ridge, newx = x_test)

# Calculate RMSE for Ridge Regression
rmse_ridge <- sqrt(mean((ridge_predictions - happiness_data_test$Life_Ladder)^2))
print(rmse_ridge)
```

# Result Interpretation
This section of the project focuses on extracting meaningful insights from the trained models, primarily by examining the importance of features. The goal is to understand which predictors have the most impact on predicting happiness levels.

## Multiple Linear Regression Feature Importance
In the Multiple Linear Regression model, feature importance is assessed using t-values, which are extracted, sorted, and ranked. 'Healthy life expectancy' emerges as the most significant predictor, followed closely by 'Social support' and 'Positive affect.' 'Perceptions of corruption' also shows a notable impact, while 'Freedom to make life choices' and 'Negative affect' have comparatively lower significance.  

```{r}
##################### Step 4: Result Interpretation ###########################

### Multiple Linear Regression Feature Importance ###

# Extract t-values from the multiple linear regression model
summary_multiple_linear <- summary(model_multiple_linear)
t_values <- summary_multiple_linear$coefficients[, "t value"] # Extract only t-values

# Create a table with Predictor and T-value
coef_table_multiple_linear <- data.frame(
  Predictor = rownames(summary_multiple_linear$coefficients),
  T_Value = t_values,
  Abs_T_Value = abs(t_values) # Absolute T-values for ranking
)

# Exclude the intercept
coef_table_multiple_linear <- coef_table_multiple_linear[coef_table_multiple_linear$Predictor != "(Intercept)", ]

# Sort the table by absolute T-value (descending order) to get feature importance ranking
coef_table_sorted_multiple_linear <- coef_table_multiple_linear[order(-coef_table_multiple_linear$Abs_T_Value), ]

# Show sorted table with T-values
cat("Multiple Linear Regression Feature Importance (T-Values):\n")
print(coef_table_sorted_multiple_linear)

# Visualize Feature Importance based on T-values
ggplot(coef_table_sorted_multiple_linear, aes(x = reorder(Predictor, Abs_T_Value), y = T_Value)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(x = "Predictor", y = "T-Value") +
  ggtitle("Multiple Linear Regression Feature Importance (T-Values)")


```
## Residual Analysis for the Multiple Linear Regression
This analysis checks whether residuals meet the requirements of zero mean, homoscedasticity, independence, and normality. First, the mean of residuals is examined to confirm that it is close to zero, indicating that model predictions are centered around the actual values. The Residuals vs. Fitted Values plot assesses homoscedasticity, where a random spread without patterns would confirm constant variance across fitted values. A Q-Q Plot and Histogram of Residuals provide insights into the normality of residuals, which is essential for valid t-tests and confidence intervals. Additionally, Residuals vs. Each Predictor plots check for any patterns that could indicate non-linearity or correlation between residuals and predictors, suggesting that the model may not fully capture predictor effects. Finally, a Residuals vs. Data Index plot checks for independence of residuals, ensuring there are no autocorrelations that could affect model assumptions. Together, these diagnostics confirm whether the Multiple Linear Regression model is well-specified or if adjustments are necessary to meet these assumptions.


```{r}
############### Multiple Linear Regression Residual Analysis ###############

# 1. Calculate Residuals and Fitted Values
residuals_multiple_linear <- residuals(model_multiple_linear)
fitted_values_multiple_linear <- fitted(model_multiple_linear)

# 2. Check Zero Mean of Residuals
mean_residuals <- mean(residuals_multiple_linear)
cat("Mean of residuals:", mean_residuals, "\n")  # Should be close to zero

# 3. Residuals vs. Fitted Values Plot (Check Homoscedasticity)
# This plot helps check for constant variance of residuals
ggplot(data = data.frame(Fitted = fitted_values_multiple_linear, Residuals = residuals_multiple_linear), 
       aes(x = Fitted, y = Residuals)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs. Fitted Values (Check Homoscedasticity)", x = "Fitted Values", y = "Residuals") +
  theme_minimal()

# 4. Q-Q Plot of Residuals (Check Normality)
# This plot checks if residuals follow a normal distribution
qqnorm(residuals_multiple_linear)
qqline(residuals_multiple_linear, col = "red", lwd = 2)
title("Q-Q Plot of Residuals (Check Normality)")

# 5. Histogram of Residuals (Additional Check for Normality)
# This plot helps visually inspect the distribution of residuals
ggplot(data = data.frame(Residuals = residuals_multiple_linear), aes(x = Residuals)) +
  geom_histogram(binwidth = 0.5, fill = "lightblue", color = "black") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency") +
  theme_minimal()

# 6. Residuals vs. Each Predictor (Check for Independence and Linearity)
# This checks if there are any patterns in residuals against individual predictors
predictors <- c("Social_support", "Healthy_life_expectancy", "Freedom_to_make_life_choices", 
                "Perceptions_of_corruption", "Positive_affect", "Negative_affect")

# Loop through each predictor and plot residuals
for (predictor in predictors) {
  plot <- ggplot(data = data.frame(Predictor = happiness_data_train_cleaned[[predictor]], Residuals = residuals_multiple_linear), 
                 aes(x = Predictor, y = Residuals)) +
    geom_point(alpha = 0.6) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(title = paste("Residuals vs.", predictor, "(Check Linearity and Independence)"), x = predictor, y = "Residuals") +
    theme_minimal()
  print(plot)  # Print each plot
}

# 7. Residuals vs. Data Index (Check for Independence)
# This plot checks for patterns that may indicate lack of independence
data_index <- seq_along(residuals_multiple_linear)
ggplot(data = data.frame(Index = data_index, Residuals = residuals_multiple_linear), aes(x = Index, y = Residuals)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs. Data Index (Check Independence)", x = "Data Index", y = "Residuals") +
  theme_minimal()


```

## Multinomial Logistic Regression Feature Importance
The interpretation of the Multinomial Logistic Regression model reveals varying predictor importance across different 'Happiness_Level' categories. Using z-scores for ranking, each predictorâ€™s impact on the likelihood of being in a particular happiness level compared to the baseline is shown in faceted bar plots. The results indicate that "Healthy_life_expectancy" is a dominant predictor across most categories, particularly influencing 'happy,' 'unhappy,' and 'very_happy' levels. 'Social support' and 'Positive affect' also emerge as significant predictors, especially for the 'happy' and 'very_happy' categories. The plots demonstrate that different factors play crucial roles at each happiness level.

```{r}
########### Multinomial Logistic Regression Feature Importance ########

# Extract coefficients and standard errors from the multinomial logistic regression model
summary_multinomial <- summary(model_multinomial) 
coefs_multinomial <- summary_multinomial$coefficients 
std_errors_multinomial <- summary_multinomial$standard.errors 

# Create a table with coefficients, standard errors, and approximate z-scores for each class
coef_table_multinomial <- data.frame() 
for (i in 1:nrow(coefs_multinomial)) {
  coefs <- coefs_multinomial[i, ]
  std_errors <- std_errors_multinomial[i, ]
  z_scores <- coefs / std_errors  # Calculate approximate z-scores 
  coef_table <- data.frame(
    Predictor = names(coefs),
    Coefficient = coefs,
    Std_Error = std_errors,  # Include standard errors 
    Z_Score = z_scores,  # Include z-scores for ranking 
    Abs_Z_Score = abs(z_scores)  # Absolute z-scores for importance ranking
  )
  coef_table$Class <- rownames(coefs_multinomial)[i]
  coef_table_multinomial <- rbind(coef_table_multinomial, coef_table)
}

# Exclude the intercept
coef_table_multinomial <- coef_table_multinomial[coef_table_multinomial$Predictor != "(Intercept)", ]

# Deduplicate Predictor names based on Class to avoid ambiguity
coef_table_multinomial <- coef_table_multinomial %>%
  group_by(Class, Predictor) %>%
  summarise(
    Coefficient = first(Coefficient),  # Take the first occurrence (if duplicates exist)
    Std_Error = first(Std_Error),
    Z_Score = first(Z_Score),
    Abs_Z_Score = first(Abs_Z_Score)
  ) %>%
  ungroup()

# Sort the table by absolute z-score (descending order) for each class
coef_table_sorted_multinomial <- coef_table_multinomial %>%
  arrange(Class, desc(Abs_Z_Score))

# Show sorted table with Z-scores for each class
cat("Multinomial Logistic Regression Feature Importance (Z-Scores):\n")
print(coef_table_sorted_multinomial) 
 




# Visualize Feature Importance based on Z-scores for each class
ggplot(coef_table_sorted_multinomial, aes(x = reorder(Predictor, Abs_Z_Score), y = Z_Score, fill = Class)) + 
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(x = "Predictor", y = "Z-Score") + 
  ggtitle("Multinomial Logistic Regression Feature Importance (Z-Scores)") + 
  facet_wrap(~ Class, scales = "free") +
  theme(
    plot.title = element_text(size = 5),    
    axis.text = element_text(size = 3.5),    
    legend.text = element_text(size = 4)     
  )
```
# Comparative Analysis
This section of the project meticulously evaluated the performance of five models: Multiple Linear Regression, Multinomial Logistic Regression, Ridge Regression, Polynomial Regression, and Naive Bayes. The performance was measured using appropriate metrics for each model's specific type of prediction. The classification models (Multinomial Logistic Regression and Naive Bayes) were assessed on their ability to predict 'Happiness_Level,' a categorical variable with four levels, while the regression models (Multiple Linear Regression, Ridge Regression, and Polynomial Regression) were evaluated on their ability to predict the continuous 'Life_Ladder' score. Each model's results were compared to determine their suitability for the respective tasks.

```{r}
######### Comparing Polynomial Regression with Multiple Linear Regression #########

# Fit Polynomial Regression (Adding quadratic terms for selected predictors)
poly_model <- lm(Life_Ladder ~ poly(Social_support, 2) + poly(Healthy_life_expectancy, 2) + 
                 poly(Freedom_to_make_life_choices, 2) + poly(Perceptions_of_corruption, 2) +
                 poly(Positive_affect, 2) + poly(Negative_affect, 2), 
                 data = happiness_data_train_cleaned)

# Predictions for Polynomial Regression
predictions_poly <- predict(poly_model, newdata = happiness_data_test)

# Calculate RMSE for Polynomial Regression
rmse_poly <- sqrt(mean((predictions_poly - happiness_data_test$Life_Ladder)^2))

# Extract Adjusted R-squared for Polynomial Regression
adjusted_r_squared_poly <- summary(poly_model)$adj.r.squared

# Calculate AIC for Polynomial Regression
aic_poly <- AIC(poly_model)

# Print Comparison of Evaluation Metrics
cat("\nComparison of Evaluation Metrics:\n")
cat("Multiple Linear Regression:\n")
cat("  RMSE:", rmse_multiple_linear, "\n")
cat("  Adjusted R^2:", adjusted_r_squared_multiple_linear, "\n") # Adjusted R^2
cat("  AIC:", aic_multiple_linear, "\n") # AIC for Multiple Linear Regression
cat("Polynomial Regression:\n")
cat("  RMSE:", rmse_poly, "\n")
cat("  Adjusted R^2:", adjusted_r_squared_poly, "\n") # Adjusted R^2 for Polynomial Regression
cat("  AIC:", aic_poly, "\n") # AIC for Polynomial Regression
```

```{r}
############### Polynomial Regression Residual Analysis ###############

# 1. Calculate Residuals and Fitted Values
residuals_poly <- residuals(poly_model)
fitted_values_poly <- fitted(poly_model)

# 2. Check Zero Mean of Residuals
mean_residuals_poly <- mean(residuals_poly)
cat("Mean of residuals (Polynomial Regression):", mean_residuals_poly, "\n")  # Should be close to zero

# 3. Residuals vs. Fitted Values Plot (Check Homoscedasticity)
# This plot helps check for constant variance of residuals
ggplot(data = data.frame(Fitted = fitted_values_poly, Residuals = residuals_poly), 
       aes(x = Fitted, y = Residuals)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Polynomial Regression: Residuals vs. Fitted Values (Check Homoscedasticity)", 
       x = "Fitted Values", y = "Residuals") +
  theme_minimal()

# 4. Q-Q Plot of Residuals (Check Normality)
# This plot checks if residuals follow a normal distribution
qqnorm(residuals_poly)
qqline(residuals_poly, col = "red", lwd = 2)
title("Polynomial Regression: Q-Q Plot of Residuals (Check Normality)")

# 5. Histogram of Residuals (Additional Check for Normality)
# This plot helps visually inspect the distribution of residuals
ggplot(data = data.frame(Residuals = residuals_poly), aes(x = Residuals)) +
  geom_histogram(binwidth = 0.5, fill = "lightblue", color = "black") +
  labs(title = "Polynomial Regression: Histogram of Residuals (Check Normality)", 
       x = "Residuals", y = "Frequency") +
  theme_minimal()

# 6. Residuals vs. Each Predictor (Check for Independence and Linearity)
# This checks if there are any patterns in residuals against individual predictors
predictors <- c("Social_support", "Healthy_life_expectancy", "Freedom_to_make_life_choices", 
                "Perceptions_of_corruption", "Positive_affect", "Negative_affect")

# Loop through each predictor and plot residuals
for (predictor in predictors) {
  plot <- ggplot(data = data.frame(Predictor = happiness_data_train_cleaned[[predictor]], Residuals = residuals_poly), 
                 aes(x = Predictor, y = Residuals)) +
    geom_point(alpha = 0.6) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(title = paste("Polynomial Regression: Residuals vs.", predictor, "(Check Linearity and Independence)"), 
         x = predictor, y = "Residuals") +
    theme_minimal()
  print(plot)  # Print each plot
}

# 7. Residuals vs. Data Index (Check for Independence)
# This plot checks for patterns that may indicate lack of independence
data_index <- seq_along(residuals_poly)
ggplot(data = data.frame(Index = data_index, Residuals = residuals_poly), aes(x = Index, y = Residuals)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Polynomial Regression: Residuals vs. Data Index (Check Independence)", 
       x = "Data Index", y = "Residuals") +
  theme_minimal()
```


```{r}
################## Multinomial Logistic Regression vs. Naive Bayes ###############

# Train Naive Bayes Classifier
model_naive_bayes <- naiveBayes(Happiness_Level ~ Social_support + Healthy_life_expectancy +
                                  Freedom_to_make_life_choices + Perceptions_of_corruption + 
                                  Positive_affect + Negative_affect, 
                                data = happiness_data_train_cleaned)

# Predict classes for the test set
predictions_naive_bayes <- predict(model_naive_bayes, newdata = happiness_data_test)

# Create confusion matrix for Naive Bayes
confusionMatrix_naive_bayes <- table(Predicted = predictions_naive_bayes, Reference = happiness_data_test$Happiness_Level)
print(confusionMatrix_naive_bayes)

# Calculate accuracy for Naive Bayes
accuracy_naive_bayes <- mean(predictions_naive_bayes == happiness_data_test$Happiness_Level)

# Calculate Precision, Recall, and F1-Score for each class
precision_naive_bayes <- diag(confusionMatrix_naive_bayes) / rowSums(confusionMatrix_naive_bayes)
recall_naive_bayes <- diag(confusionMatrix_naive_bayes) / colSums(confusionMatrix_naive_bayes)
f1_score_naive_bayes <- 2 * precision_naive_bayes * recall_naive_bayes / (precision_naive_bayes + recall_naive_bayes)


# Convert Happiness_Level to numeric for ROC analysis
happiness_data_test$Happiness_Level_numeric <- as.numeric(happiness_data_test$Happiness_Level)

# Predict probabilities for Naive Bayes
probabilities_naive_bayes <- predict(model_naive_bayes, newdata = happiness_data_test, type = "raw")

# Initialize a list to store ROC curves
roc_curves_naive_bayes <- list()

# Plot ROC curves for each class
par(mfrow = c(2, 3)) # Set up the plotting area to have 2 rows and 3 columns
for (i in 1:nlevels(happiness_data_test$Happiness_Level)) {
  # Generate ROC curve for the current class
  roc_curves_naive_bayes[[i]] <- roc(happiness_data_test$Happiness_Level_numeric == i, probabilities_naive_bayes[, i])
  
  # Plot the ROC curve
  plot(roc_curves_naive_bayes[[i]], main = paste("ROC Curve -", levels(happiness_data_test$Happiness_Level)[i]), 
       print.auc = TRUE, col = "blue", lwd = 2)
}



############### Comparative Results Display ###############

# Create a summary table for accuracy
results_comparison <- data.frame(
  Model = c("Multinomial Logistic Regression", "Naive Bayes"),
  Accuracy = c(accuracy_multinomial, accuracy_naive_bayes)
)

# Print the summary table
cat("\nComparative Results (Accuracy):\n")
print(results_comparison)

# Combine Precision, Recall, and F1-Score for each model and class
precision_comparison <- data.frame(
  Model = c(rep("Multinomial Logistic Regression", length(precision_multinomial)),
            rep("Naive Bayes", length(precision_naive_bayes))),
  Class = rep(levels(happiness_data_test$Happiness_Level), 2),
  Precision = c(precision_multinomial, precision_naive_bayes)
)

recall_comparison <- data.frame(
  Model = c(rep("Multinomial Logistic Regression", length(recall_multinomial)),
            rep("Naive Bayes", length(recall_naive_bayes))),
  Class = rep(levels(happiness_data_test$Happiness_Level), 2),
  Recall = c(recall_multinomial, recall_naive_bayes)
)

f1_score_comparison <- data.frame(
  Model = c(rep("Multinomial Logistic Regression", length(f1_score_multinomial)),
            rep("Naive Bayes", length(f1_score_naive_bayes))),
  Class = rep(levels(happiness_data_test$Happiness_Level), 2),
  F1_Score = c(f1_score_multinomial, f1_score_naive_bayes)
)

# Print class-wise comparisons
cat("\nClass-wise Precision Comparison:\n")
print(precision_comparison)

cat("\nClass-wise Recall Comparison:\n")
print(recall_comparison)

cat("\nClass-wise F1-Score Comparison:\n")
print(f1_score_comparison)

```



# Example Input and Output
```{r}
############### Example Input and Output: Multiple Linear Regression ###############

# Example Input (using the first row from the test set as an example)
example_input <- happiness_data_test[1, c("Social_support", "Healthy_life_expectancy", 
                                          "Freedom_to_make_life_choices", "Perceptions_of_corruption", 
                                          "Positive_affect", "Negative_affect")]

# Convert to a data frame for prediction
example_input_df <- data.frame(example_input)

# Predict the output (Life Ladder) using the Multiple Linear Regression model
example_prediction <- predict(model_multiple_linear, newdata = example_input_df)

# Display Example Input
cat("Example Input:\n")
print(example_input_df)

# Display Predicted Output
cat("\nPredicted Output (Life Ladder):", example_prediction, "\n")

# Optionally, show the actual output for comparison
actual_output <- happiness_data_test$Life_Ladder[1]
cat("\nActual Output (Life Ladder):", actual_output, "\n")

```
```{r}
############### Example Input and Output: Multinomial Logistic Regression ###############

# Example Input (using the second row from the test set as an example)
example_input <- happiness_data_test[8, c("Social_support", "Healthy_life_expectancy", 
                                          "Freedom_to_make_life_choices", "Perceptions_of_corruption", 
                                          "Positive_affect", "Negative_affect")]

# Convert to a data frame for prediction
example_input_df <- data.frame(example_input)

# Predict the probabilities for each class using the Multinomial Logistic Regression model
example_probabilities <- predict(model_multinomial, newdata = example_input_df, type = "probs")

# Predict the most likely class
example_predicted_class <- predict(model_multinomial, newdata = example_input_df)

# Display Example Input
cat("Example Input:\n")
print(example_input_df)

# Display Predicted Probabilities
cat("\nPredicted Probabilities for Each Class:\n")
print(round(example_probabilities, 3))

# Display Predicted Class
cat("\nPredicted Class (Happiness Level):", example_predicted_class, "\n")

# Show the actual output for comparison 
actual_output <- happiness_data_test$Happiness_Level[1]
cat("\nActual Class (Happiness Level):", actual_output, "\n")

```


```{r}
############### Example Input and Output: Ridge Regression ###############

# Example Input (using the third row from the test set as an example)
example_input <- happiness_data_test[3, c("Social_support", "Healthy_life_expectancy", 
                                          "Freedom_to_make_life_choices", "Perceptions_of_corruption", 
                                          "Positive_affect", "Negative_affect")]

# Convert to a model matrix for Ridge Regression
example_input_matrix <- model.matrix(~ ., data = example_input)[, -1]

# Predict the output (Life Ladder) using the Ridge Regression model
example_prediction <- predict(ridge_model_best, s = best_lambda_ridge, newx = example_input_matrix)

# Display Example Input
cat("Example Input:\n")
print(data.frame(example_input))

# Display Predicted Output
cat("\nPredicted Output (Life Ladder):", example_prediction, "\n")

# Show the actual output for comparison
actual_output <- happiness_data_test$Life_Ladder[1]
cat("\nActual Output (Life Ladder):", actual_output, "\n")

```
# Model Comparison

***RMSE (Root Mean Squared Error):*** The regression models, including Multiple Linear Regression, Ridge Regression, and Polynomial Regression, which predict the continuous Life_Ladder score, were evaluated using RMSE. The Multiple Linear Regression model achieved an RMSE of approximately 0.59. Ridge Regression had a similar RMSE of 0.58. Polynomial Regression slightly deteriorated the RMSE to 0.54. The classification models (Multinomial Logistic Regression and Naive Bayes) cannot be evaluated using RMSE as they predict categorical outcomes.\

***Accuracy:*** The classification models, Multinomial Logistic Regression, and Naive Bayes were evaluated with accuracy. The Multinomial Logistic Regression model achieved an accuracy of 0.65. Naive Bayes achieved a similar accuracy of 0.64. The regression models do not directly predict categories and thus cannot be evaluated using accuracy.\

***Precision, Recall, and F1-Score:*** These metrics were calculated for each Happiness_Level in the classification models. For Multinomial Logistic Regression, the very_happy class exhibited precision of 0.77, and a recall of 0.69. The other classes also showed a balance between precision and recall, with F1 scores ranging from 0.61 to 0.72. For Naive Bayes, precision, recall, and F1 scores varied across classes, with F1 scores ranging from 0.60 to 0.72. The regression models do not produce class-specific predictions, so these metrics are not applicable.\

***AUC (Area Under the ROC Curve):*** The discriminatory power of the Multinomial Logistic Regression model was assessed using ROC curves and AUC values for each class. The AUC values ranged from 0.82 to 0.94. For Naive Bayes, the AUC values were similar, ranging from 0.82 to 0.93. The regression models do not produce probabilistic predictions for multiple classes, so AUC is not relevant for them.\

***Best Performing Model:***
In the context of predicting the continuous Life_Ladder score, Polynomial Regression demonstrated the best performance with a slightly lower RMSE of 0.54 compared to Multiple Linear Regression, and Ridge Regression, all of which achieved an RMSE of approximately 0.59 and 0.58. Ridge Regression might still be preferred if multicollinearity is a concern, as it is specifically designed to handle such situations.\

For predicting the categorical Happiness_Level, the Multinomial Logistic Regression model showed better overall performance with an accuracy of 0.65, slightly outperforming Naive Bayes, which had an accuracy of 0.64.


# Conclusion
The project employed five models: Multiple Linear Regression, Multinomial Logistic Regression, Ridge Regression, Polynomial Regression, and Naive Bayes, to predict national happiness. The regression models (Multiple Linear Regression, Ridge Regression, and Polynomial Regression) aimed to predict the continuous Life_Ladder score, while the classification models (Multinomial Logistic Regression and Naive Bayes) predicted the categorical Happiness_Level. The models' performance was evaluated using various metrics, including RMSE for continuous prediction and accuracy, precision, recall, and F1-score for categorical prediction.\

Among the regression models, Polynomial Regression demonstrated the best performance with a slightly improved RMSE of 0.54, compared to 0.59 for Multiple Linear Regression, and 0.58 for Ridge Regression. For classification, the Multinomial Logistic Regression model achieved an accuracy of 0.65 in predicting Happiness_Level, outperforming Naive Bayes, which achieved an accuracy of 0.64.\

The project's findings highlight the strengths and limitations of these models, emphasizing the importance of selecting an appropriate model based on the specific context and objectives. Further analysis using different datasets could provide more definitive insights into the relative strengths and weaknesses of these models for national happiness prediction.\
